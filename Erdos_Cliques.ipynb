{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Erdos goes neural: Maximum Clique \n",
    "\n",
    "## You can use this notebook to reproduce the maximum clique results on the Twitter, IMDB, and COLLAB datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#cd\n",
    "%config Completer.use_jedi = False\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\faraz\\.conda\\envs\\pyG\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#imports\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Linear\n",
    "from itertools import product\n",
    "import time\n",
    "from torch import tensor\n",
    "from torch.optim import Adam\n",
    "from torch.optim import SGD\n",
    "from math import ceil\n",
    "from torch.nn import Linear\n",
    "from torch.distributions import categorical\n",
    "from torch.distributions import Bernoulli\n",
    "import torch.nn\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "from torch_geometric.utils import convert as cnv\n",
    "from torch_geometric.utils import sparse as sp\n",
    "from torch_geometric.data import Data\n",
    "from networkx.drawing.nx_agraph import graphviz_layout\n",
    "import networkx as nx\n",
    "from torch.utils.data.sampler import RandomSampler\n",
    "from torch.nn.functional import gumbel_softmax\n",
    "from torch.distributions import relaxed_categorical\n",
    "import myfuncs\n",
    "from torch_geometric.nn.inits import uniform\n",
    "from torch_geometric.nn.inits import glorot, zeros\n",
    "from torch.nn import Parameter\n",
    "from torch.nn import Sequential as Seq, Linear, ReLU\n",
    "from torch_geometric.nn import MessagePassing\n",
    "from torch_geometric.utils import degree\n",
    "from torch_geometric.nn import GINConv, GATConv, global_mean_pool, NNConv, GCNConv\n",
    "from torch.nn import Parameter\n",
    "from torch.nn import Sequential as Seq, Linear, ReLU, LeakyReLU\n",
    "from torch_geometric.nn import MessagePassing\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Linear, Sequential, ReLU, BatchNorm1d as BN\n",
    "from torch_geometric.nn import GINConv, global_mean_pool\n",
    "from torch_geometric.data import Batch \n",
    "from torch_scatter import scatter_min, scatter_max, scatter_add, scatter_mean\n",
    "from torch import autograd\n",
    "from torch_geometric.utils import to_dense_batch, to_dense_adj\n",
    "from torch_geometric.utils import softmax, add_self_loops, remove_self_loops, segregate_self_loops, remove_isolated_nodes, contains_isolated_nodes, add_remaining_self_loops\n",
    "from torch_geometric.utils import dropout_adj, to_undirected, to_networkx\n",
    "from torch_geometric.utils import is_undirected\n",
    "from cut_utils import get_diracs\n",
    "import scipy\n",
    "import scipy.io\n",
    "from matplotlib.lines import Line2D\n",
    "from torch_geometric.utils.convert import from_scipy_sparse_matrix\n",
    "import GPUtil\n",
    "from networkx.algorithms.approximation import max_clique\n",
    "import pickle\n",
    "from torch_geometric.nn import SplineConv, global_mean_pool, DataParallel\n",
    "from torch_geometric.data import DataListLoader, DataLoader\n",
    "from random import shuffle\n",
    "from networkx.algorithms.approximation import max_clique\n",
    "from networkx.algorithms import graph_clique_number\n",
    "from networkx.algorithms import find_cliques\n",
    "from torch_geometric.nn.norm import graph_size_norm\n",
    "from torch_geometric.datasets import TUDataset\n",
    "import visdom \n",
    "from visdom import Visdom \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from  cut_utils import solve_gurobi_maxclique\n",
    "import gurobipy as gp\n",
    "from gurobipy import GRB\n",
    "from models import clique_MPNN\n",
    "from torch_geometric.nn.norm.graph_size_norm import GraphSizeNorm\n",
    "from modules_and_utils import decode_clique_final, decode_clique_final_speed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\faraz\\.conda\\envs\\pyG\\lib\\site-packages\\torch_geometric\\deprecation.py:12: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    }
   ],
   "source": [
    "datasets = [\"TWITTER_SNAP\", \"COLLAB\", \"IMDB-BINARY\"]\n",
    "dataset_name = datasets[0]\n",
    "#datasetname = \"COLLAB_shuffle_1\"\n",
    "#datasetname = \"TWITTER_SNAP\"\n",
    "#dataset_name = \"IMDB-BINARY\"\n",
    "\n",
    " #TUDataset(root='/tmp/ENZYMES', name='ENZYMES')\n",
    "\n",
    "if dataset_name == \"TWITTER_SNAP\":\n",
    "    stored_dataset = open('datasets/TWITTER_SNAP_2.p', 'rb')\n",
    "    dataset = pickle.load(stored_dataset)\n",
    "elif dataset_name == \"COLLAB\":\n",
    "    dataset = TUDataset(root='/tmp/'+dataset_name, name=dataset_name)\n",
    "    #stored_dataset = open('datasets/dataset_shuffle_1.p', 'rb')\n",
    "elif dataset_name == \"IMDB-BINARY\":\n",
    "    #stored_dataset = open('datasets/IMDB_BINARY.p', 'rb')\n",
    "    dataset = TUDataset(root='/tmp/'+dataset_name, name=dataset_name)\n",
    "\n",
    "dataset_scale = 1\n",
    "total_samples = int(np.floor(len(dataset)*dataset_scale))\n",
    "dataset = dataset[:total_samples]\n",
    "\n",
    "num_trainpoints = int(np.floor(0.6*len(dataset)))\n",
    "\n",
    "\n",
    "num_valpoints = int(np.floor(num_trainpoints/3))\n",
    "num_testpoints = len(dataset) - (num_trainpoints + num_valpoints)\n",
    "\n",
    "\n",
    "traindata= dataset[0:num_trainpoints]\n",
    "valdata = dataset[num_trainpoints:num_trainpoints + num_valpoints]\n",
    "testdata = dataset[num_trainpoints + num_valpoints:]\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "train_loader = DataLoader(traindata, batch_size, shuffle=True)\n",
    "test_loader = DataLoader(testdata, batch_size, shuffle=False)\n",
    "val_loader =  DataLoader(valdata, batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "#set up random seeds \n",
    "torch.manual_seed(1)\n",
    "np.random.seed(2)   \n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Parameters:  613958\n",
      "Total trainable parameters:  613958\n"
     ]
    }
   ],
   "source": [
    "#number of propagation layers\n",
    "numlayers = 5\n",
    "\n",
    "#size of receptive field\n",
    "receptive_field = numlayers + 1\n",
    "\n",
    "# val_losses = []\n",
    "# cliq_dists = []\n",
    "\n",
    "net =  clique_MPNN(dataset,numlayers, 32, 32,1)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "lr_decay_step_size = 5\n",
    "lr_decay_factor = 0.95\n",
    "\n",
    "net.to(device).reset_parameters()\n",
    "optimizer = Adam(net.parameters(), lr=0.001, weight_decay=0.00)\n",
    "\n",
    "total_params = sum(p.numel() for p in net.parameters())\n",
    "total_trainable_params = sum(p.numel() for p in net.parameters() if p.requires_grad)\n",
    "\n",
    "\n",
    "print(\"Total Parameters: \", total_params)\n",
    "print(\"Total trainable parameters: \", total_trainable_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pyg_data_list(data_list):\n",
    "    pyg_data_l = []\n",
    "    for d in data_list:\n",
    "        pyg_data_l.append(Data(x=d['x'], edge_index=d['edge_index']))\n",
    "\n",
    "    return pyg_data_l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Edge_dropout:  0.0\n",
      "Penalty_coefficient:  4.0\n",
      "here2\n",
      "Epoch:  0\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 80% | 32% |\n",
      "here2\n",
      "Epoch:  1\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 24% | 63% |\n",
      "here2\n",
      "Epoch:  2\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 27% | 72% |\n",
      "here2\n",
      "Epoch:  3\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 37% | 73% |\n",
      "here2\n",
      "Epoch:  4\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 41% | 76% |\n",
      "Edge_dropout:  0.0\n",
      "here2\n",
      "Epoch:  5\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 38% | 77% |\n",
      "here2\n",
      "Epoch:  6\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 37% | 77% |\n",
      "here2\n",
      "Epoch:  7\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 36% | 77% |\n",
      "here2\n",
      "Epoch:  8\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 40% | 77% |\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32md:\\CNDP-RL\\erdos_neu\\Erdos_Cliques.ipynb Cell 9'\u001b[0m in \u001b[0;36m<cell line: 45>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/CNDP-RL/erdos_neu/Erdos_Cliques.ipynb#ch0000007?line=97'>98</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad(), \n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/CNDP-RL/erdos_neu/Erdos_Cliques.ipynb#ch0000007?line=98'>99</a>\u001b[0m data \u001b[39m=\u001b[39m data\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m--> <a href='vscode-notebook-cell:/d%3A/CNDP-RL/erdos_neu/Erdos_Cliques.ipynb#ch0000007?line=99'>100</a>\u001b[0m data_prime \u001b[39m=\u001b[39m get_diracs(data, \u001b[39m1\u001b[39;49m, sparse \u001b[39m=\u001b[39;49m \u001b[39mTrue\u001b[39;49;00m, effective_volume_range\u001b[39m=\u001b[39;49m\u001b[39m0.15\u001b[39;49m, receptive_field \u001b[39m=\u001b[39;49m receptive_field)\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/CNDP-RL/erdos_neu/Erdos_Cliques.ipynb#ch0000007?line=101'>102</a>\u001b[0m data \u001b[39m=\u001b[39m data\u001b[39m.\u001b[39mto(\u001b[39m'\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/CNDP-RL/erdos_neu/Erdos_Cliques.ipynb#ch0000007?line=102'>103</a>\u001b[0m data_prime \u001b[39m=\u001b[39m data_prime\u001b[39m.\u001b[39mto(device)\n",
      "File \u001b[1;32md:\\CNDP-RL\\erdos_neu\\cut_utils.py:122\u001b[0m, in \u001b[0;36mget_diracs\u001b[1;34m(data, N, n_diracs, sparse, flat, replace, receptive_field, effective_volume_range, max_iterations, complement)\u001b[0m\n\u001b[0;32m    113\u001b[0m                 gr, gc \u001b[39m=\u001b[39m graph_edge_index\n\u001b[0;32m    115\u001b[0m \u001b[39m#                 print(\"Gr: \", gr)\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[39m#                 print(\"Graph edge index: \", graph_edge_index)\u001b[39;00m\n\u001b[0;32m    117\u001b[0m \u001b[39m#                 print(\"gr max: \", gr.max())\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    120\u001b[0m \n\u001b[0;32m    121\u001b[0m                 \u001b[39m#get dirac\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m                 randInt \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mchoice(\u001b[39mrange\u001b[39;49m(graph_nodes), N, replace \u001b[39m=\u001b[39m replace)\n\u001b[0;32m    123\u001b[0m                 node_sample \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mzeros(N\u001b[39m*\u001b[39mgraph_nodes,device\u001b[39m=\u001b[39mdevice)\n\u001b[0;32m    124\u001b[0m                 offs  \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39marange(N, device\u001b[39m=\u001b[39mdevice)\u001b[39m*\u001b[39mgraph_nodes\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Temporarily here\n",
    "# epochs = 200\n",
    "# net.train()\n",
    "# retdict = {}\n",
    "# edge_drop_p = 0.0\n",
    "# edge_dropout_decay = 0.90\n",
    "# penalty_coeff = 9.00\n",
    "# penalty_increase = -0.00\n",
    "# validation_timeout = 500\n",
    "\n",
    "# b_sizes = [32]\n",
    "# l_rates = [0.001]\n",
    "# depths = [3]\n",
    "# coefficients = [3.5]\n",
    "# rand_seeds = [68]\n",
    "# widths = [64]\n",
    "\n",
    "#THEBEST\n",
    "# epochs = 100\n",
    "# net.train()\n",
    "# retdict = {}\n",
    "# edge_drop_p = 0.0\n",
    "# edge_dropout_decay = 0.90\n",
    "# penalty_coeff = 9.00\n",
    "# penalty_increase = -0.00\n",
    "# validation_timeout = 75\n",
    "\n",
    "\n",
    "\n",
    "b_sizes = [32]\n",
    "l_rates = [0.001]\n",
    "depths = [4]\n",
    "coefficients = [4.]\n",
    "rand_seeds = [66]\n",
    "widths = [64]\n",
    "\n",
    "epochs = 100\n",
    "net.train()\n",
    "retdict = {}\n",
    "edge_drop_p = 0.0\n",
    "edge_dropout_decay = 0.90\n",
    "\n",
    "\n",
    "\n",
    "for batch_size, learning_rate, numlayers, penalty_coeff, r_seed, hidden_1 in product(b_sizes, l_rates, depths, coefficients, rand_seeds, widths):\n",
    "   \n",
    "    torch.manual_seed(r_seed)\n",
    "\n",
    "    traindata_pyg = get_pyg_data_list(traindata)\n",
    "    testdata_pyg = get_pyg_data_list(testdata)\n",
    "    valdata_pyg = get_pyg_data_list(valdata)\n",
    "\n",
    "    train_loader = DataLoader(traindata_pyg, batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(testdata_pyg, batch_size, shuffle=False)\n",
    "    val_loader =  DataLoader(valdata_pyg, batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "    receptive_field= numlayers + 1\n",
    "    val_losses = []\n",
    "    cliq_dists = []\n",
    "\n",
    "    #hidden_1 = 128\n",
    "    hidden_2 = 1\n",
    "\n",
    "    net =  clique_MPNN(dataset,numlayers, hidden_1, hidden_2 ,1)\n",
    "    net.to(device).reset_parameters()\n",
    "    optimizer = Adam(net.parameters(), lr=learning_rate, weight_decay=0.00000)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        totalretdict = {}\n",
    "        count=0\n",
    "        if epoch % 5 == 0:\n",
    "            edge_drop_p = edge_drop_p*edge_dropout_decay\n",
    "            print(\"Edge_dropout: \", edge_drop_p)\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "            penalty_coeff = penalty_coeff + 0.\n",
    "            print(\"Penalty_coefficient: \", penalty_coeff)\n",
    "\n",
    "        print(\"here2\")\n",
    "        #learning rate schedule\n",
    "        if epoch % lr_decay_step_size == 0:\n",
    "            for param_group in optimizer.param_groups:\n",
    "                        param_group['lr'] = lr_decay_factor * param_group['lr']\n",
    "\n",
    "        #show currrent epoch and GPU utilizationss\n",
    "        print('Epoch: ', epoch)\n",
    "        GPUtil.showUtilization()\n",
    "\n",
    "\n",
    "\n",
    "        #print(\"here3\")\n",
    "\n",
    "\n",
    "        net.train()\n",
    "        for data in train_loader:\n",
    "            count += 1 \n",
    "            optimizer.zero_grad(), \n",
    "            data = data.to(device)\n",
    "            data_prime = get_diracs(data, 1, sparse = True, effective_volume_range=0.15, receptive_field = receptive_field)\n",
    "\n",
    "            data = data.to('cpu')\n",
    "            data_prime = data_prime.to(device)\n",
    "\n",
    "\n",
    "            retdict = net(data_prime, None, penalty_coeff)\n",
    "\n",
    "            for key,val in retdict.items():\n",
    "                if \"sequence\" in val[1]:\n",
    "                    if key in totalretdict:\n",
    "                        totalretdict[key][0] += val[0].item()\n",
    "                    else:\n",
    "                        totalretdict[key] = [val[0].item(),val[1]]\n",
    "\n",
    "            if epoch > 2:\n",
    "                    retdict[\"loss\"][0].backward()\n",
    "                    #reporter.report()\n",
    "\n",
    "                    torch.nn.utils.clip_grad_norm_(net.parameters(),1)\n",
    "                    optimizer.step()\n",
    "                    del(retdict)\n",
    "\n",
    "        if epoch > -1:        \n",
    "            for key,val in totalretdict.items():\n",
    "                if \"sequence\" in val[1]:\n",
    "                    val[0] = val[0]/(len(train_loader.dataset)/batch_size)\n",
    "            del data_prime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get ground truths from Gurobi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Academic license - for non-commercial use only - expires 2021-05-20\n",
      "Using license file /nfs_home/karalias/gurobi.lic\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_data_clique = []\n",
    "for data in testdata:\n",
    "    my_graph = to_networkx(Data(x=data.x, edge_index = data.edge_index)).to_undirected()\n",
    "    print(my_graph)\n",
    "    cliqno, _ = solve_gurobi_maxclique(my_graph, 500)\n",
    "    data.clique_number = cliqno\n",
    "    test_data_clique += [data]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data prep and fp:  0.10942673683166504\n",
      "Derandomization time:  0.4599459171295166\n",
      "data prep and fp:  0.1107785701751709\n",
      "Derandomization time:  0.3966653347015381\n",
      "data prep and fp:  0.11343550682067871\n",
      "Derandomization time:  0.41412973403930664\n",
      "data prep and fp:  0.11215519905090332\n",
      "Derandomization time:  0.35886597633361816\n",
      "data prep and fp:  0.11196208000183105\n",
      "Derandomization time:  0.3746011257171631\n",
      "data prep and fp:  0.11066675186157227\n",
      "Derandomization time:  0.4072113037109375\n",
      "data prep and fp:  0.11163878440856934\n",
      "Derandomization time:  0.39858007431030273\n",
      "data prep and fp:  0.10869216918945312\n",
      "Derandomization time:  0.4298062324523926\n",
      "Current batch:  1\n",
      "Time so far:  4.153944969177246\n",
      "data prep and fp:  0.11052346229553223\n",
      "Derandomization time:  0.49470949172973633\n",
      "data prep and fp:  0.11177825927734375\n",
      "Derandomization time:  0.5036251544952393\n",
      "data prep and fp:  0.11250662803649902\n",
      "Derandomization time:  0.5123715400695801\n",
      "data prep and fp:  0.1129448413848877\n",
      "Derandomization time:  0.5438504219055176\n",
      "data prep and fp:  0.11321115493774414\n",
      "Derandomization time:  0.504326343536377\n",
      "data prep and fp:  0.1120004653930664\n",
      "Derandomization time:  0.5014138221740723\n",
      "data prep and fp:  0.11308979988098145\n",
      "Derandomization time:  0.5836424827575684\n",
      "data prep and fp:  0.10947012901306152\n",
      "Derandomization time:  0.42089319229125977\n",
      "Current batch:  2\n",
      "Time so far:  4.98507285118103\n",
      "data prep and fp:  0.1178743839263916\n",
      "Derandomization time:  0.44054126739501953\n",
      "data prep and fp:  0.11174941062927246\n",
      "Derandomization time:  0.4759054183959961\n",
      "data prep and fp:  0.11192536354064941\n",
      "Derandomization time:  0.4352428913116455\n",
      "data prep and fp:  0.11133813858032227\n",
      "Derandomization time:  0.476381778717041\n",
      "data prep and fp:  0.10853242874145508\n",
      "Derandomization time:  0.4198188781738281\n",
      "data prep and fp:  0.1461191177368164\n",
      "Derandomization time:  0.41941165924072266\n",
      "data prep and fp:  0.11186337471008301\n",
      "Derandomization time:  0.464125394821167\n",
      "data prep and fp:  0.1117854118347168\n",
      "Derandomization time:  0.46851420402526855\n",
      "Current batch:  3\n",
      "Time so far:  4.562853813171387\n",
      "data prep and fp:  0.1121668815612793\n",
      "Derandomization time:  0.5949673652648926\n",
      "data prep and fp:  0.11184883117675781\n",
      "Derandomization time:  0.6150949001312256\n",
      "data prep and fp:  0.11159205436706543\n",
      "Derandomization time:  0.600794792175293\n",
      "data prep and fp:  0.1094973087310791\n",
      "Derandomization time:  0.6810989379882812\n",
      "data prep and fp:  0.14069223403930664\n",
      "Derandomization time:  0.6327569484710693\n",
      "data prep and fp:  0.11263561248779297\n",
      "Derandomization time:  0.5872735977172852\n",
      "data prep and fp:  0.11242818832397461\n",
      "Derandomization time:  0.5697920322418213\n",
      "data prep and fp:  0.11232686042785645\n",
      "Derandomization time:  0.617804765701294\n",
      "Current batch:  4\n",
      "Time so far:  5.848385572433472\n",
      "data prep and fp:  0.11984920501708984\n",
      "Derandomization time:  0.5160398483276367\n",
      "data prep and fp:  0.10809159278869629\n",
      "Derandomization time:  0.5204105377197266\n",
      "data prep and fp:  0.11050963401794434\n",
      "Derandomization time:  0.5017378330230713\n",
      "data prep and fp:  0.1126859188079834\n",
      "Derandomization time:  0.5494883060455322\n",
      "data prep and fp:  0.11231780052185059\n",
      "Derandomization time:  0.5216419696807861\n",
      "data prep and fp:  0.11357522010803223\n",
      "Derandomization time:  0.4997591972351074\n",
      "data prep and fp:  0.11269021034240723\n",
      "Derandomization time:  0.49639105796813965\n",
      "data prep and fp:  0.10875964164733887\n",
      "Derandomization time:  0.531231164932251\n",
      "Current batch:  5\n",
      "Time so far:  5.058855056762695\n",
      "data prep and fp:  0.11519742012023926\n",
      "Derandomization time:  0.5113935470581055\n",
      "data prep and fp:  0.11157393455505371\n",
      "Derandomization time:  0.49852585792541504\n",
      "data prep and fp:  0.1096041202545166\n",
      "Derandomization time:  0.5476622581481934\n",
      "data prep and fp:  0.10872769355773926\n",
      "Derandomization time:  0.4833242893218994\n",
      "data prep and fp:  0.11092972755432129\n",
      "Derandomization time:  0.46988916397094727\n",
      "data prep and fp:  0.11045360565185547\n",
      "Derandomization time:  0.49112534523010254\n",
      "data prep and fp:  0.10872983932495117\n",
      "Derandomization time:  0.4426388740539551\n",
      "data prep and fp:  0.11278104782104492\n",
      "Derandomization time:  0.46462154388427734\n",
      "Current batch:  6\n",
      "Time so far:  4.821763277053833\n",
      "data prep and fp:  0.02484440803527832\n",
      "Derandomization time:  0.042414188385009766\n",
      "data prep and fp:  0.028929948806762695\n",
      "Derandomization time:  0.053822994232177734\n",
      "data prep and fp:  0.022550106048583984\n",
      "Derandomization time:  0.04374098777770996\n",
      "data prep and fp:  0.021895170211791992\n",
      "Derandomization time:  0.0566103458404541\n",
      "data prep and fp:  0.019711017608642578\n",
      "Derandomization time:  0.06244087219238281\n",
      "data prep and fp:  0.019668102264404297\n",
      "Derandomization time:  0.056587934494018555\n",
      "data prep and fp:  0.022559642791748047\n",
      "Derandomization time:  0.05500221252441406\n",
      "data prep and fp:  0.019644498825073242\n",
      "Derandomization time:  0.06150531768798828\n",
      "Current batch:  7\n",
      "Time so far:  0.6205272674560547\n",
      "Average time per graph:  0.1536327868091817\n"
     ]
    }
   ],
   "source": [
    "tbatch_size = batch_size\n",
    "num_data_points = num_testpoints\n",
    "\n",
    "batch_size = 32\n",
    "test_data = testdata\n",
    "test_loader = DataLoader(test_data, batch_size, shuffle=False)\n",
    "net.to(device)\n",
    "count = 1\n",
    "\n",
    "#Evaluation on test set\n",
    "net.eval()\n",
    "\n",
    "gnn_nodes = []\n",
    "gnn_edges = []\n",
    "gnn_sets = {}\n",
    "\n",
    "#set number of samples according to your execution time, for 10 samples\n",
    "max_samples = 8\n",
    "\n",
    "gnn_times = []\n",
    "num_samples = max_samples\n",
    "t_start = time.time()\n",
    "\n",
    "for data in test_loader:\n",
    "    num_graphs = data.batch.max().item()+1\n",
    "    bestset = {}\n",
    "    bestedges = np.zeros((num_graphs))\n",
    "    maxset = np.zeros((num_graphs))\n",
    "\n",
    "    total_samples = []\n",
    "    for graph in range(num_graphs):\n",
    "        curr_inds = (data.batch==graph)\n",
    "        g_size = curr_inds.sum().item()\n",
    "        if max_samples <= g_size: \n",
    "            samples = np.random.choice(curr_inds.sum().item(),max_samples, replace=False)\n",
    "        else:\n",
    "            samples = np.random.choice(curr_inds.sum().item(),max_samples, replace=True)\n",
    "\n",
    "        total_samples +=[samples]\n",
    "\n",
    "    data = data.to(device)\n",
    "    t_0 = time.time()\n",
    "\n",
    "    for k in range(num_samples):\n",
    "        t_datanet_0 = time.time()\n",
    "        data_prime = get_diracs(data.to(device), 1, sparse = True, effective_volume_range=0.15, receptive_field = 7)\n",
    "  \n",
    "        initial_values = data_prime.x.detach()\n",
    "        data_prime.x = torch.zeros_like(data_prime.x)\n",
    "        g_offset = 0\n",
    "        for graph in range(num_graphs):\n",
    "            curr_inds = (data_prime.batch==graph)\n",
    "            g_size = curr_inds.sum().item()\n",
    "            graph_x = data_prime.x[curr_inds]\n",
    "            data_prime.x[total_samples[graph][k] + g_offset]=1.\n",
    "            g_offset += g_size\n",
    "            \n",
    "        retdz = net(data_prime)\n",
    "        \n",
    "        t_datanet_1 = time.time() - t_datanet_0\n",
    "        print(\"data prep and fp: \", t_datanet_1)\n",
    "        t_derand_0 = time.time()\n",
    "\n",
    "        sets, set_edges, set_cardinality = decode_clique_final_speed(data_prime,(retdz[\"output\"][0]), weight_factor =0.,draw=False, beam = 1)\n",
    "\n",
    "        t_derand_1 = time.time() - t_derand_0\n",
    "        print(\"Derandomization time: \", t_derand_1)\n",
    "\n",
    "        for j in range(num_graphs):\n",
    "            indices = (data.batch == j)\n",
    "            if (set_cardinality[j]>maxset[j]):\n",
    "                    maxset[j] = set_cardinality[j].item()\n",
    "                    bestset[str(j)] = sets[indices].cpu()\n",
    "                    bestedges[j] = set_edges[j].item()\n",
    "\n",
    "    t_1 = time.time()-t_0\n",
    "    print(\"Current batch: \", count)\n",
    "    print(\"Time so far: \", time.time()-t_0)\n",
    "    gnn_sets[str(count)] = bestset\n",
    "    \n",
    "    gnn_nodes += [maxset]\n",
    "    gnn_edges += [bestedges]\n",
    "    gnn_times += [t_1]\n",
    "\n",
    "    count += 1\n",
    "\n",
    "t_1 = time.time()\n",
    "total_time = t_1 - t_start\n",
    "print(\"Average time per graph: \", total_time/(len(test_data)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#flatten output\n",
    "flat_list = [item for sublist in gnn_edges for item in sublist]\n",
    "for k in range(len(flat_list)):\n",
    "    flat_list[k] = flat_list[k].item()\n",
    "gnn_edges = (flat_list)\n",
    "\n",
    "flat_list = [item for sublist in gnn_nodes for item in sublist]\n",
    "for k in range(len(flat_list)):\n",
    "    flat_list[k] = flat_list[k].item()\n",
    "gnn_nodes = (flat_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean ratio: 0.9404887155480641 +/-  0.08857525456359257\n"
     ]
    }
   ],
   "source": [
    "tests = test_data_clique\n",
    "ratios = [gnn_nodes[i]/tests[i].clique_number for i in range(len(tests))]\n",
    "print(f\"Mean ratio: {(np.array(ratios)).mean()} +/-  {(np.array(ratios)).std()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('pyG')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "5820f3b57864765b2bc287cac460df818c0f4e0a59fb741a5e96a8fe0bfc6f4e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
